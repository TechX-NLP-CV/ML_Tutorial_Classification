{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Machine Learning Tutorial: Classification</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is Classification?\n",
    "Classification is the problem to correctly predict the class of the given input data. Unlike the regression problem (which could predict a specifict value), the classification aims to tell which category the data belongs to.\n",
    "\n",
    "There are generally 4 types of different classification problems:\n",
    "\n",
    "1. Binary Classification: this could only predict the result from 2 possible classes. For example, you might want to classify an email as **spam** or **not spam** depending on the content.\n",
    "\n",
    "2. Multi-Class Classification: this could predict the result from more than 2 possible classes. For example, you might want to know the type of fruits from **apple**, **pear**, **orange** and so on.\n",
    "\n",
    "3. Multi-Label Classification: this could predict more than 1 labels for each data. For example, within a movie classification task, a movie might be simultaneously belongs to lots of different types of labels like **Action**, **Adventure**, **Fantasy** and you want to get 3 labels for every movie you want to predict.\n",
    "\n",
    "4. Imbalanced Classification: this means that during training process the distribution of examples for each category is not consistent.\n",
    "\n",
    "Within this tutorial, we will explore the binary classification deeply and help you understand more about the supervised learning and the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What is Supervised Learning?\n",
    "Supervised learning is the machine learning task to learn a mapping between the input featuers and the output and the goal is to ***generalise from the training data to accurately find the result for unseen data.***\n",
    "\n",
    "Within the classification, we would like to know the relationship between the ***features*** and the ***class label***. In other words, we want to train the model so it could generally classify ***objects with 4 wheels*** as a ***car.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Training, Validation and Test Sets (Very Important!!!) (You could skip this part if you have seen this in other tutorials)\n",
    "As we have seen, the reason why machine learning algorithms could have this fancy performance is that generally they use a very large amount of data to \"learn\" how to solve the problem.\n",
    "\n",
    "Within this \"learning\" process, we could divide data into training, validation adn test datasets for different purpose:\n",
    "\n",
    "1. Training set ***(could \"see\", could \"use\" in the training)***: The model could access to this dataset to optimise the weights and do calculations in the training.\n",
    "\n",
    "2. Validation set ***(could \"see\", can't \"use\" in the training)***: During the training, the model could only use the validation set to determine how good it could perform.\n",
    "\n",
    "3. Test set: ***(can't \"see\", can't \"use\" in the training)***: After training, the model could use the test set to determine how good it could perform on unseen data.\n",
    "\n",
    "The validation set and test set are very similar as they are both used for evaluating the accuracy of the model. However, the key difference is that (very important!!!):\n",
    "\n",
    "1. ***Validation set could be used during the training process, which means you could observe the loss information and tune the hyperparameters but can't use it for optimisation and calculation.***\n",
    "\n",
    "2. ***Test set could only be used after the training is finished. It is used to check how the model could perform on absolutely unseen data. In other words, you can't do any modification to the model once you use the test set.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Linear Classifier (Binary)\n",
    "Let's consider the below graph:\n",
    "\n",
    "1. This is a 2D feature space, which means that each point within this space has 2 features, $x_1$ and $x_2$.\n",
    "\n",
    "2. Within our case, we could simply define $y=sign(\\theta\\cdot x)$, which could output $1$ if the dot product between the model weight $\\theta$ and the input data $x$ is positive. Otherwise it could output $-1$ if the dot product is negative.\n",
    "\n",
    "3. Graphically, the model could be represented as the $\\theta$ vector and we define the ***decision boundary*** as ***the line perpendicular to that vector***. If the input data falls into the purple area (in which the dot product is always greater than 0), it is classified to be ***class A***. The same principle could be applied to the blue area, where points are classified as ***class B***.\n",
    "\n",
    "5. By adjusting the model weight $\\theta$, we are actually changing the gradient of the decision boundary. By allowing the offset $\\theta _0$, we could modify the intercept of the decision boundary.\n",
    "\n",
    "6. If the dot product is 0, which means that the point lies exactly on the decision boundary, we can't give it a valid class.\n",
    "\n",
    "We will come back to the margin boundary later.\n",
    "\n",
    "![Binary Classification](https://miro.medium.com/max/1400/1*sfqR-rZSYaITJ5_PbXU_9w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Classification Error\n",
    "We could use the classification erro to determine how good our classifier could perform.\n",
    "\n",
    "This is simply defined as ***the ratio between the number of wrong predictions and the number of total preditions***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Perceptron Algorithm\n",
    "##### 3.4.1 Matrix Definition\n",
    "Before we dive into the perceptron algorithm, let us express our problem in the matrix form, which could be calculated efficiently.\n",
    "\n",
    "Within the linear classification problem (binary), we have the following components:\n",
    "\n",
    "1. N different labels as a (N,1) matrix Y where each label could only be chosen from {-1, 1}\n",
    "\n",
    "$$\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{bmatrix}$$\n",
    "\n",
    "2. N different x values with d features as a (N,d+1) matrix $X$\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "1 & x_{1,1} & x{1,2} & \\dots & x_{1,d} \\\\\n",
    "1 & x_{2,1} & x{2,2} & \\dots & x_{2,d} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\dots & \\vdots \\\\\n",
    "1 & x_{N,1} & x{N,2} & \\dots & x_{N,d} \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "we use $x_i$ to represent the $i^{th}$ row of this matrix, which is a (1, d+1) vector.\n",
    "\n",
    "3. $\\theta$ as (d+1, 1) matrix\n",
    "\n",
    "$$\\begin{bmatrix} \\theta _0 \\\\ \\theta _1 \\\\ \\vdots \\\\ \\theta _N \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4.2 Detailed Algorithm\n",
    "The whole algorithm contains the following steps:\n",
    "\n",
    "1. Initialise our $\\theta$ as a vector full of 0.\n",
    "\n",
    "2. Use every point in the training set from 1 to N:\n",
    "\n",
    "    a. Check whether the classifier could give a correct prediction for the point i by determing $y_i(\\theta\\cdot x_i)\\leqslant 0$ (If the prediction is inconsistent with the label, their product must be negative)\n",
    "\n",
    "    b. If it is incorrect, update the model by using $\\theta _{new}=\\theta + y_ix_i^T$\n",
    "\n",
    "3. Repeat the step 2 until reaching the maximum number of epoch.\n",
    "\n",
    "It could be seen from the above that this is a very simple algorithm and the core idea is that ***only updating the model awhen it can't give a corret prediction.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4.3 Model Update\n",
    "You might wonder why we use $\\theta _{new}=\\theta + y_ix_i^T$ to update the model and whether it is efficient.\n",
    "\n",
    "Before updating, we have $y_i(\\theta\\cdot x_i)$\n",
    "\n",
    "After updating, we have $y_i(\\theta _{new}\\cdot x_i)=y_i[(\\theta + y_ix_i^T)\\cdot x_i]$\n",
    "\n",
    "If we calculate the difference between the updated value and the original value, we could get $y_i[(\\theta + y_ix_i^T)\\cdot x_i]-y_i(\\theta\\cdot x_i)=y_i^2\\lVert x_i \\rVert^2 >0$\n",
    "\n",
    "This means that our updated model could always have a greater chance to pass the check and make the prediction and true label consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.1 Distance between a Point and a Line\n",
    "Before we talk about the margin and the margin boundary, let us revise the equation for calculating the distance between a point and a line.\n",
    "\n",
    "If we have a line $\\theta\\cdot x=0$ and an arbitrary point $x_0$, the distance between them is $\\frac{\\left\\lvert \\theta\\cdot x_0 \\right\\rvert}{\\lVert \\theta \\rVert}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.2 Margin and Margin Boundary\n",
    "Let's check the following graph.\n",
    "\n",
    "You might notice that you could find lots of decision boundary that could perfectly classify those \"+\" and \"-\" points. But is there a best one we could get?\n",
    "\n",
    "Here, we define:\n",
    "\n",
    "1. Support Vector: the point within the class that is the closest to the decision boundary\n",
    "\n",
    "2. Margin: the distance between the support vector and the decision boundary\n",
    "\n",
    "3. Margin Boundary: the straight line that is parallel to the decision boundary and the distance between them is the margin\n",
    "\n",
    "If the margin of a decision boundary is greater, that means it has a greater strength to classfy those points and generally could do better when predicting unseen data.\n",
    "\n",
    "This means that **by introducing the margin and margin boundary, it could improve the generalisation of the classifier so it has a greater potential to perform well on any unseen data.***\n",
    "\n",
    "The margin of a decision boundary is mathematically defined as $d_{sv}=\\frac{y_{sv}\\theta\\cdot x_{sv}}{\\lVert \\theta \\rVert}$ where $x_{sv}$ represents the support vector and $y_{sv}$ represents the corresponding class label.\n",
    "\n",
    "However, our $\\theta$ is highly scalable as we are only interested in the sign of the product. To simplify our calculation, we aim to find the $\\theta$ that could make $y_{sv}\\theta\\cdot x_{sv}=1$.\n",
    "\n",
    "By doing this, our margin could be finally expressed as $d_{sv}=\\frac{1}{\\lVert \\theta \\rVert}$\n",
    "\n",
    "![Binary Classification](https://uk.mathworks.com/discovery/support-vector-machine/_jcr_content/mainParsys/image.adapt.full.medium.jpg/1630399098268.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.3 Hinge Loss and Objective Function\n",
    "After introducing the concept of margin, our objective has changed to ***train a model with a high accuracy while keeping the maximum margin at the same time.***\n",
    "\n",
    "To achieve this goal, we will use a different loss function, hinge loss.\n",
    "\n",
    "The hinge loss is defined as $Hinge(\\gamma _i)=0$ if $\\gamma _i\\geqslant 1$ and $Hinge(\\gamma _i)=1-\\gamma _i$ if $\\gamma _i<1$ where $\\gamma _i=d_{i}/d_{sv}$, the ratio between the distance from point i to the decision boundary and the margin.\n",
    "\n",
    "It would be easier to think about this in 2 directions:\n",
    "\n",
    "1. If a point could be correctly classified and the distance is greater than the margin, we don't update the model.\n",
    "\n",
    "2. If a point exceeds the margin or it couldn't be correctly classified, we update the model depending on its position.\n",
    "\n",
    "By using the definition of the margin, it is obvious that $\\gamma _i=\\frac{y_i\\theta\\cdot x_i}{\\lVert \\theta \\rVert}\\cdot\\lVert \\theta \\rVert=y_i\\theta\\cdot x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.4 Empirical Risk (You could skip this part if you have seen this in other tutorials)\n",
    "Although we could train our model to work well on our training/validation sets, we absolutely have no idea how it could perform on the real unseen data.\n",
    "\n",
    "To tackle this problem, we have to use the principle of empirical risk minimisation here.\n",
    "\n",
    "Empirical risk could estimate how the model might perform on the unseen data using the accuracy on the observed dataset.\n",
    "\n",
    "Empirical risk is mathematically defined as: $R(model)=\\frac{1}{N}\\sum_{i=1}^{N} Loss(y_i , model(x_i))$\n",
    "\n",
    "Within our case, the empirical risk could be represented as $R(\\theta)=\\frac{1}{N}\\sum_{i=1}^{N} Hinge(y_i\\theta\\cdot x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.5 Regularisation (You could skip this part if you have seen this in other tutorials)\n",
    "Regularisation could be considered as a resistance for model to perfectly fit the training data.\n",
    "\n",
    "If our model could perfectly fit the training data (very low training loss), this means that it could also fit the noise within the training set very well and generally can't predict the result for unseen data.\n",
    "\n",
    "By adding the regularisation term, only a very big change could update the model, which makes it more robust and stable.\n",
    "\n",
    "Within this tutorial, we will focus on the L2 Regularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.6 Objective Function\n",
    "Our final objective function is defined as $J_ {\\lambda, N}(\\theta)=R(\\theta)+\\frac{\\lambda}{2} \\lVert \\theta \\rVert_2^2=\\frac{1}{N}\\sum_{i=1}^{N} Hinge(y_i\\theta\\cdot x_i)+\\frac{\\lambda}{2}\\sum_{i=0}^N \\theta _i ^2 $\n",
    "\n",
    "The first term is the average loss term, which could train the model to get a high accuracy.\n",
    "\n",
    "The second term is the regularisation term, which could improve the generalisation of the model and perform well on unseen data.\n",
    "\n",
    "The hyperparameter $\\lambda$ is the regularisation coefficient, which could balance the above effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.7 Gradient Descent: Concept (Very Important!!!) (You could skip this part if you have seen it in other tutorials)\n",
    "Gradient descent is a very efficient and powerful method used during the optimisation in the machine learning algorithm.\n",
    "\n",
    "The goal of optimisation is ***to find the local minimum (global minimum if you are super lucky) of the loss function to get a more accurate model***.\n",
    "\n",
    "Imagine you are on a mountain and you want to go back to the bottom of the mountain. Here is a list of what you might do:\n",
    "\n",
    "1. Find the path pointing downwards, which is the inverse way of the path you climbed up ***(Find the negative of the gradient of the loss function at the certain point).***\n",
    "\n",
    "2. Go down towards the buttom by one step ***(Update the loss by using multiplying the negative gradient and the learning rate).***\n",
    "\n",
    "3. Repeat step 1 and 2 until you reach the buttom ***(For each epoch, find the new gradient and update the loss using that).***\n",
    "\n",
    "We will explain this more in the gradient-based method part later.\n",
    "\n",
    "![Gradient Descent](https://miro.medium.com/max/1400/1*G1v2WBigWmNzoMuKOYQV_g.png \"Gradient Descent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.8 Gradient Descent Based Method\n",
    "We could use gradient descent based method instead as it could save a lot of time while keeping a relatively high accuracy.\n",
    "\n",
    "The gradient descent based method contains the following main steps:\n",
    "\n",
    "1. Initialise our $\\theta$ as a vector full of 0.\n",
    "\n",
    "2. Use every point in the training set from 1 to N:\n",
    "\n",
    "    a. Check the sign of $Hinge(y_i\\theta\\cdot x_i)$\n",
    "\n",
    "    b. Update the model according to the result of step 2(a)\n",
    "\n",
    "3. Repeat the step 2 until reaching the maximum number of epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5.9 Detailed Gradient Descent Method Analysis\n",
    "The general equation to optimize the model is: $\\theta=\\theta - rate\\cdot\\nabla _\\theta R(\\theta)$\n",
    "\n",
    "Our objective function is:  $J_ {\\lambda, N}(\\theta)=R(\\theta)+\\frac{\\lambda}{2} \\lVert \\theta \\rVert_2^2=\\frac{1}{N}\\sum_{i=1}^{N} Hinge(y_i\\theta\\cdot x_i)+\\frac{\\lambda}{2}\\sum_{i=0}^N \\theta _i ^2 $\n",
    "\n",
    "Theoretically, we need to use the entire dataset to compute gradient accurately and optimize the model. However, this could be painful if we have lots of data.\n",
    "\n",
    "Alternatively, we could estimate the gradient by calculating the accurate gradient at a specific randomly chosen point, which is known as stochastic gradient descent (SGD).\n",
    "\n",
    "This method could significantly decrease the computational cost and it makes lots of machine learning and deep learning algrotihms powerful.\n",
    "\n",
    "If we use SGD, our objective function is simplified into $Hinge(y_i\\theta\\cdot x_i)+\\frac{\\lambda}{2}\\sum_{i=0}^N \\theta _i ^2 $\n",
    "\n",
    "The first derivative of hinge loss with respect to $\\theta$ is:\n",
    "\n",
    "1. 0 if the hinge loss is less than 0\n",
    "\n",
    "2. $-y_ix_i^T$ if the hinge loss is greater than 0\n",
    "\n",
    "So the first derivate of the simplified objective function with respect to $\\theta$ is: \n",
    "\n",
    "1. $\\lambda\\theta$ if the hinge loss is less than 0\n",
    "\n",
    "2. $\\lambda\\theta - y_ix_i^T$ if the hinge loss is greater than 0\n",
    "\n",
    "Thus, we could update our $\\theta$ according to the hinge loss:\n",
    "\n",
    "1. $\\theta _{new}=\\theta-rate\\cdot\\lambda\\theta$ if the hinge loss is less than 0\n",
    "\n",
    "2. $\\theta _{new}=\\theta-rate\\cdot\\lambda\\theta+ y_ix_i^T$ if the hinge loss is greater than 0\n",
    "\n",
    "The biggest difference between the perceptron algorithm and the SVM algorithm is that ***the SVM algorithm will still update the model even it could correctly classify the current point.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Non-Linear Classification\n",
    "We have seen some efficient algorithms for solving the linear classification problems. However, there are still lots of difficult tasks that those algorithms are not able to solve.\n",
    "\n",
    "Luckily, we could extend those algorithms to non-linear cases by introducing a higher feature space and using the kernel trick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.6.1 Feature Space\n",
    "The easiest way to extend to the non-linear cases is to add more features to each point in the dataset. Generally, the resulted feautre space is much higher than the original feature space.\n",
    "\n",
    "Let's check the following plot:\n",
    "\n",
    "1. Originally we have 1-D data point: $x_i=[x]$ and they are not linearly separable in the 1-D feature space. This means that you can't find a point as the decision boundary which could perfectly classify all data points without error.\n",
    "\n",
    "2. If we simply extend those points into 2-D feature space by adding a feature $x_i=[x, x^2]$, they could be very easily linearly separated in the new feature space! This is equivalent to ***each data point could provide more information to the model so they could be more easily classified.***\n",
    "\n",
    "Alongside this, you have lots of choices for those new features, which makes the decision boundary much more flexible. But more importantly, ***finding a non-linear decision boundary in the original feature space is equivalent to finding a linear decision boundary in the higher order feature space so our linear classification algorithms could still be used here!***\n",
    "\n",
    "![featureExtension](https://lh6.googleusercontent.com/ioIfK_hzhstoNi2gMnIkB5jKDtVoUNthKPpRDqKyn_5fDt58pPfSA17E5SgQok2XYkYjc93VRL7FHNVFMz8Gis7ILOvnsdF9Mr52D4xQ_J2ekTmHd3VJC6SfbtI0gGll6ldmoBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.6.2 Kernel Trick\n",
    "Extending to a much higher feature space also means that the computational cost would be much higher.\n",
    "\n",
    "To solve this problem, we introduce the kernel trick here which doesn't require the exact feature mapping and in turn save lots of space and time.\n",
    "\n",
    "Assume we have an originally 2-D data point and we extend it using a 2-D polynormial feature transform, this could be represented as:\n",
    "\n",
    "$x_{old}=[x_1, x_2]$ is transformed to $x_{new}=[x_1, x_2, x_1^2, \\sqrt 2x_1x_2, x_2^2]$\n",
    "\n",
    "Let's compute the dot product $a_{new}\\cdot b_{new}=(a_1b_1+a_2b_2)+(a_1^2b_1^2+2a_1a_2b_1b_2+a_2^2b_2^2)=(a_1b_1+a_2b_2)+(a_1b_1+a_2b_2)^2=a_{old}\\cdot b_{old}+(a_{old}\\cdot b_{old})^2$\n",
    "\n",
    "This means that we could always find a relationship $f()$ such that $a_{new}\\cdot b_{new}=f(a_{old}\\cdot b_{old})$\n",
    "\n",
    "This is very convinient as you don't have to physically construct a high-order vector and do computationally expensive calculations but still enjoy the advantage (making the non-linear classifier possible).\n",
    "\n",
    "Such a relationship between the old and new dot product is called the ***kernel function***.\n",
    "\n",
    "In real life, we commonly use polynomial kernel function, Gaussian RBF kernel function, Sigmoid kernel function and so on. The general rule to choose the kernel function is ***always try the simplest one first and the Gaussian RBF kernel could generally perform well.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 Kernel Perceptron Algorithm\n",
    "##### 3.7.1 Apply Feature Transform\n",
    "By applying what we have seen so far, we could easily modify the perceptron algorithm and extend it into the kernel perceptron algorithm.\n",
    "\n",
    "Let's recall from the original perceptron algorithm first:\n",
    "\n",
    "1. Initialise our $\\theta$ as a vector full of 0.\n",
    "\n",
    "2. Use every point in the training set from 1 to N:\n",
    "\n",
    "    a. Check whether the classifier could give a correct prediction for the point i by determing $y_i(\\theta\\cdot x_i)\\leqslant 0$\n",
    "\n",
    "    b. If it is incorrect, update the model by using $\\theta _{new}=\\theta + y_ix_i^T$\n",
    "\n",
    "3. Repeat the step 2 until reaching the maximum number of epoch.\n",
    "\n",
    "If we define our higher feature transform as $\\phi()$, our perceptron algorithm could be easily extended to:\n",
    "\n",
    "1. Initialise our $\\theta$ as a vector full of 0.\n",
    "\n",
    "2. Use every point in the training set from 1 to N:\n",
    "\n",
    "    a. Check whether the classifier could give a correct prediction for the point i by determing $y_i\\theta\\cdot \\phi(x_i)\\leqslant 0$\n",
    "    \n",
    "    b. If it is incorrect, update the model by using $\\theta _{new}=\\theta + y_i\\phi(x_i^T)$\n",
    "\n",
    "3. Repeat the step 2 until reaching the maximum number of epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.7.2 Apply Kernel Trick\n",
    "If we look at the above modified algorithm, we could easily spot the truth that: $\\theta _{final}=integer*y_i\\phi(x_i^T)$ where this integer is the total number of update using the data point i during the whole training process.\n",
    "\n",
    "Consider a simple case where we only have 1 data point in the training set and we update the model using this point every epoch within the total 10 epochs. This means that $\\theta _{final}=10*y\\phi(x^T)$\n",
    "\n",
    "According to this fact, we could define our model in a new way:\n",
    "\n",
    "1. We could record the number of update the model is updated by using this point within the whole training process as $\\alpha$\n",
    "\n",
    "2. So our new model could be represented as $\\theta _{final}=\\sum_{j=1}^{N} \\alpha _jy_j\\phi(x_j^T)$, which is simply the sum of all update using all points.\n",
    "\n",
    "3. Finally, $\\theta\\cdot \\phi(x_i)=\\sum_{j=1}^{N} \\alpha _jy_j\\phi(x_j^T)\\cdot \\phi(x_i)=\\sum_{j=1}^{N} \\alpha _jy_j Kernel(x_j, x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.7.3 Final Result\n",
    "So, we could have our kernel perceptron algorithm:\n",
    "\n",
    "1. Initialise our $\\theta$ as a vector full of 0. \n",
    "\n",
    "2. Initialise the number of update for each point $(\\alpha _1, \\alpha _2, \\cdots, \\alpha _N)$ all as 0.\n",
    "\n",
    "2. Use every point in the training set from 1 to N:\n",
    "\n",
    "    a. Check whether the classifier could give a correct prediction for the point i by determing $y_i\\sum_{j=1}^{N} \\alpha _jy_j Kernel(x_j, x_i)\\leqslant 0$\n",
    "\n",
    "    b. If it is incorrect, update the model by using $\\alpha _i=\\alpha _i+1$\n",
    "\n",
    "3. Repeat the step 2 until reaching the maximum number of epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn is a very popular API for machine learning in Python. It contains lots of convinient functions to help us build our own pipeline.\n",
    "\n",
    "Within this tutorial, we will implement the original perceptron algorithm, the SVM and the kernel perceptron algorithm from scratch and using scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Perceptron Algorithm from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Dataset Loading\n",
    "Before we dive into the specific algorithm, let's first load the dataset we are going to use.\n",
    "\n",
    "The dataset we use in this tutorial is the breast cancer wisconsin dataset, which is a very classic and easy binary classification dataset.\n",
    "\n",
    "This dataset contains 569 tumors in total and each tumor contains 30 different features.\n",
    "\n",
    "Every tumor could be either classified as \"Maglignant\" or \"Benign\".\n",
    "\n",
    "For more information about those tumor features, please check: https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "def loadBreastCancer():\n",
    "    '''\n",
    "       Load the breast cancer dataset and return points\n",
    "    and labels separately\n",
    " \n",
    "    Return:\n",
    "        dataset: np.ndarray, all tumors with 30 features, (the number of tumors, the number of features for each tumor)\n",
    "        label: np.ndarray, all corresponding labels, (the number of tumors, 1)\n",
    "    '''\n",
    "    # load_breast_cancer() could directly load the dataset into the required form\n",
    "    # For more information, please check: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\n",
    "    dataset, label=load_breast_cancer(return_X_y=True)\n",
    "    \n",
    "    # Here we reshape the label so it is a 2D array instead of 1D\n",
    "    # np.reshape() could be used to change the shape of the array\n",
    "    # \"-1\" means to be calculated by the parameter at this position is calculated automatically\n",
    "    # For more information, please check: https://numpy.org/doc/stable/reference/generated/numpy.reshape.html\n",
    "    return dataset, label\n",
    "    #return dataset, label.reshape(-1 ,1)\n",
    "\n",
    "dataset, label=loadBreastCancer()\n",
    "print(f\"There are {dataset.shape[0]} tumors within the dataset and each tumor has {dataset.shape[1]} features.\")\n",
    "print(f\"There are also {label.shape[0]} corresponding labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every feature within the dataset has its own scale (minimum and maximum value), this would cause a serious problem in our later training process.\n",
    "\n",
    "One very efficient and important pre-processing is to scale those features to zero-mean with unit variance.\n",
    "\n",
    "This could be simply achieved by using the StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standardScale(dataset):\n",
    "    '''\n",
    "        Scale down the dataset so they share the same scale\n",
    "\n",
    "        For more information, please check: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "    \n",
    "    Argument:\n",
    "        dataset: np.ndarray, all tumors with 30 features, (the number of tumors, the number of features for each tumor)\n",
    "\n",
    "    Return:\n",
    "        result: np.ndarray, the scaled dataset, (the number of tumors, the number of features for each tumor)\n",
    "    '''\n",
    "    scaler=StandardScaler()\n",
    "    return scaler.fit_transform(dataset)\n",
    "\n",
    "dataset, label=loadBreastCancer()\n",
    "scaledDataset=standardScale(dataset)\n",
    "print(\"The original first point in the dataset is:\")\n",
    "print(dataset[:1])\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "print(\"The scaled first point in the dataset is:\")\n",
    "print(scaledDataset[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also as mentioned in the section 3.4.1, our previous analysis and conclusion are only valid when the label is chosen from {-1, 1}.\n",
    "\n",
    "So we need to modify the loaded label and repalce every 0 with -1 so they could be used for those hand-written algorithms later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def modifyLabel(label):\n",
    "    '''\n",
    "        Modify the label by replacing every 0 with -1\n",
    "    \n",
    "    Argument:\n",
    "        label: np.ndarray, all corresponding labels, (the number of tumors, 1)\n",
    "    \n",
    "    Return:\n",
    "        result: np.ndarray, the modified labels containing only -1 and 1, (the number of tumors, 1)\n",
    "    '''\n",
    "    # np.copy() could be used to deeply copy an array, instead of just the memory address\n",
    "    # Here we do want to keep the original label\n",
    "    # For more information, please check: https://numpy.org/doc/stable/reference/generated/numpy.copy.html\n",
    "    result=np.copy(label)\n",
    "    # This magic indexing allows us to easily find every index corresponding to \"0\" and assign it to \"-1\"\n",
    "    result[result==0]=-1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to insert a column of \"1\" at the beginning of dataset to change its shape into (N, d+1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def addColumn(dataset):\n",
    "    '''\n",
    "        Add a column of \"1\" to the beginning of the dataset\n",
    "    to change the shape into (N, d+1)\n",
    "\n",
    "    Argument:\n",
    "        dataset: np.ndarray, all tumors with 30 features, (the number of tumors, the number of features for each tumor)\n",
    "    \n",
    "    Return:\n",
    "        result: np.ndarray, dataset with an extra column, (the number of tumors, the number of features for each tumor+1)\n",
    "    '''\n",
    "    # np.ones() could be used to construct an array full of 1 with the given shape\n",
    "    # For more information, please check: https://numpy.org/doc/stable/reference/generated/numpy.ones.html\n",
    "    column=np.ones((1, dataset.shape[0]))\n",
    "    \n",
    "    # np.insert() could be used to insert the column at the specific position\n",
    "    # obj=0 means we want to insert at the index 0\n",
    "    # axis=1 means we want to insert a column\n",
    "    # For more information, please check: https://numpy.org/doc/stable/reference/generated/numpy.insert.html?highlight=insert#numpy.insert\n",
    "    return np.insert(dataset, 0, column, axis=1)\n",
    "\n",
    "dataset, label=loadBreastCancer()\n",
    "modifiedDataset=addColumn(dataset)\n",
    "print(f\"After modification, there are {modifiedDataset.shape[0]} tumors now and each tumor has {modifiedDataset.shape[1]} features.\")\n",
    "print(f\"The first feature value of the tumor is: {modifiedDataset[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Zero Initialisation\n",
    "We need to initialise our $\\theta$ matrix, which is full of 0 with shape (d+1, 1)\n",
    "\n",
    "However, this function could also be used to initialise the number of update for each point, which is full of 0 with shape (N, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialiseZero(number):\n",
    "    '''\n",
    "        Initialise and return the theta matrix according to the shape of the dataset\n",
    "    \n",
    "        This could be used for:\n",
    "        1. weight initialisation, (the number of features for each tumor+1, 1)\n",
    "        2. alpha initialisation, (the number of tumors, 1)\n",
    "\n",
    "    Argument:\n",
    "        number: int, determines the number of 0s in the result\n",
    "    \n",
    "    Return:\n",
    "        result: np.ndarray, full of 0, (number, 1)\n",
    "    '''\n",
    "    # np.zeros could be used to construct an array full of 0 with the given shape\n",
    "    # For more information, please check: https://numpy.org/doc/stable/reference/generated/numpy.zeros.html?highlight=zeros#numpy.zeros\n",
    "    return np.zeros((number, 1))\n",
    "\n",
    "dataset, label=loadBreastCancer()\n",
    "modifiedDataset=addColumn(dataset)\n",
    "theta=initialiseZero(modifiedDataset.shape[1])\n",
    "print(f\"The shape of the initial theta is: {theta.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Check Condition\n",
    "We could use $y_i(\\theta\\cdot x_i)\\leqslant 0$ to check whether the classifier could correctly classify the point i.\n",
    "\n",
    "This condition is true if the prediciton is false.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict(point, theta):\n",
    "    '''\n",
    "        Find the prediction by taking the sign of the dot product\n",
    "    between the point and the theta\n",
    "\n",
    "    Argument:\n",
    "        point: np.ndarray, arbitrary number of tumors with 31 features, (arbitrary number, the number of features for each tumor+1)\n",
    "        theta: np.ndarray, represents the classifier, (the number of features for each tumor+1, 1)\n",
    "\n",
    "    Return:\n",
    "        result: np.ndarray, the corresponding prediction, (arbitrary number, 1)\n",
    "    '''\n",
    "    # np.dot() could be used to find the dot product for 1-D input and matrix multiplication for 2-D input\n",
    "    # For more information, please check: https://numpy.org/doc/stable/reference/generated/numpy.dot.html\n",
    "    prediction=np.dot(point, theta)\n",
    "    # np.sign() could be used to find the sign of each value\n",
    "    # For more information, please check: https://numpy.org/doc/stable/reference/generated/numpy.sign.html\n",
    "    return np.sign(prediction)\n",
    "\n",
    "def checkPrediction(point, label, theta):\n",
    "    '''\n",
    "        Check whether the classifier could correctly classify\n",
    "    the given point\n",
    "\n",
    "    Argument:\n",
    "        point: np.ndarray, the tumor with 30 features, (1, the number of features for each tumor+1)\n",
    "        label: int, the corresponding label of the tumor, either -1 or 1\n",
    "        theta: np.ndarray, represents the classifier in the perceptron algorithm, (the number of features for each tumor+1, 1)\n",
    "    \n",
    "    Return:\n",
    "        result: boolean, True if the classifier could give a correct prediction, False otherwise\n",
    "    '''\n",
    "    prediction=predict(point, theta)\n",
    "    \n",
    "    # Here we strictly want to return a boolean type object\n",
    "    if label*prediction>0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Model Update\n",
    "We could update the $\\theta$ by using $\\theta _{new}=\\theta + y_ix_i^T$ if the classifier gives a wrong prediction for the point i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def updateThetaPerceptron(point, label, theta):\n",
    "    '''\n",
    "        Update the theta by using the point and the label\n",
    "    \n",
    "    Argument:\n",
    "        point: np.ndarray, the tumor with 30 features, (1, the number of features for each tumor+1)\n",
    "        label: int, the corresponding label of the tumor, either -1 or 1\n",
    "        theta: np.ndarray, represents the classifier in the perceptron algorithm, (the number of features for each tumor+1, 1)\n",
    "    \n",
    "    Return:\n",
    "        result: np.ndarray, udpated theta, (the number of features for each tumor+1, 1)\n",
    "    '''\n",
    "    return theta+label*point.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Classification Error\n",
    "The classification error is simply the ratio between ***the number of wrong predictions*** and ***the total number of predictions***.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(prediction, label):\n",
    "    '''\n",
    "        Calculate the classification error by using the prediction and\n",
    "    the truth label\n",
    "\n",
    "    Argument:\n",
    "        prediction: np.ndarray, the prediction by the classifier, (arbitrary number, the number of features for each tumor+1)\n",
    "        label: np.ndarray, the corresponding truth label, (arbitrary number, 1)\n",
    "\n",
    "    Return: \n",
    "        error: float, the classification error \n",
    "    '''\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85f7d4b76af0bdc033882a17c4ab285a68052c0160eb92b69fd8bca0b7c220f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
